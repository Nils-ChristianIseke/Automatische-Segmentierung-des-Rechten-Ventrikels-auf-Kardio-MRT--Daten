{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "interpreter": {
      "hash": "337c2964203791ddd05d8a9bcbaad51658a661dc83181747efd8fe6209ca158b"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('engelhardt': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "process.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Anwendung von Deep Learning-Verfahren in der Medizinischen Bildanalyse \n",
        "## Automatische Segmentierung des Rechten Ventrikels auf Kardio-MRT-Daten (MICCAI Challenge 2017)"
      ],
      "metadata": {
        "id": "g_6Cn3_NL3fx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports \n",
        "Sry for the pain in the eyes\n"
      ],
      "metadata": {
        "id": "z73hrTGm7Egm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.   Install requiered modules\n",
        "!pip install -U plotly\n",
        "!pip install SimpleITK --quiet\n",
        "!pip install --upgrade batchgenerators --quiet\n",
        "!pip install opencv-python\n",
        "!pip3 install medpy"
      ],
      "metadata": {
        "id": "JL_3R1KyuYBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import natsort\n",
        "import os\n",
        "from batchgenerators.utilities.file_and_folder_operations import *\n",
        "import SimpleITK as sitk\n",
        "\n",
        "\n",
        "from multiprocessing import Pool\n",
        "\n",
        "\n",
        "class CreateNumpyArr():\n",
        "    def __init__(self,srcDir,destinationFolder,subfolders,norm_method=None):\n",
        "        self.srcDir = srcDir\n",
        "        self.destinationFolder = destinationFolder\n",
        "        self.subfolders = subfolders\n",
        "        self.norm_method = norm_method\n",
        "        self.list_of_files = self.get_list_of_files()\n",
        "        self.createNpArr()\n",
        "        \n",
        "    def createNpArr(self,num_threads =1):\n",
        "        list_of_lists = self.list_of_files\n",
        "        folderDst =self.destinationFolder\n",
        "        maybe_mkdir_p(folderDst)\n",
        "        patient_names = [i[0][0].split(\"/\")[-1][0:10] for i in list_of_lists]\n",
        "        p = Pool(processes=1)\n",
        "        p.starmap(self.load_and_preprocess, zip(list_of_lists, patient_names, [folderDst] * len(list_of_lists)))\n",
        "        p.close()\n",
        "        p.join()\n",
        "    def get_list_of_files(self):\n",
        "        \"\"\"\n",
        "        returns a list of lists containing the filenames. The outer list contains all training examples. Each entry in the\n",
        "        outer list is again a list pointing to the files of that training example in the following order:\n",
        "        MRT, Segmentation\n",
        "        :param srcDir: \n",
        "        :type srcDir: str \n",
        "        :return: lists of files\n",
        "        \"\"\"\n",
        "        list_of_lists = []\n",
        "\n",
        "        for type in self.subfolders:\n",
        "            current_directory = os.path.join(self.srcDir, type)\n",
        "            patients = subfolders(current_directory, join=False)\n",
        "            for p in patients:\n",
        "                image = []\n",
        "                segmentation = []\n",
        "                for file in os.listdir(os.path.join(current_directory,p,\"image\")):\n",
        "                    image.append(os.path.join(current_directory,p,\"image\",file))\n",
        "                for file in os.listdir(os.path.join(current_directory,p,\"segmentation\")):\n",
        "                    segmentation.append(os.path.join(current_directory,p,\"segmentation\",file))\n",
        "                \n",
        "                image=natsort.natsorted(image)\n",
        "                segmentation = natsort.natsorted(segmentation)\n",
        "                list_of_lists.append([image,segmentation])\n",
        "\n",
        "\n",
        "                # assert all((isfile(i) for i in this_case)), \"some file is missing for patient %s; make sure the following \" \\\n",
        "                #                                             \"files are there: %s\" % (p, str(this_case))\n",
        "        print(\"Found %d patients\" % len(list_of_lists))\n",
        "        return list_of_lists\n",
        "\n",
        "\n",
        "\n",
        "    def load_and_preprocess(self,case, patient_name, output_folder):\n",
        "        \"\"\"\n",
        "        loads, preprocesses and saves a case\n",
        "        This is what happens here:\n",
        "        1) load all images and stack them to a 4d array\n",
        "        2) crop to nonzero region, this removes unnecessary zero-valued regions and reduces computation time\n",
        "        3) normalize the nonzero region with its mean and standard deviation\n",
        "        4) save 4d tensor as numpy array. Also save metadata required to create niftis again (required for export\n",
        "        of predictions)\n",
        "        :param case:\n",
        "        :param patient_name:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        \n",
        "        # load SimpleITK Images\n",
        "        imgs_sitk = [sitk.ReadImage(i) for i in case]\n",
        "           # get some metadata\n",
        "        spacing = imgs_sitk[0].GetSpacing()\n",
        "        \n",
        "        direction = imgs_sitk[0].GetDirection()\n",
        "        origin = imgs_sitk[0].GetOrigin()\n",
        "\n",
        "        # get pixel arrays from SimpleITK images\n",
        "        imgs_npy = [sitk.GetArrayFromImage(i) for i in imgs_sitk]\n",
        "        \n",
        "        \n",
        "        # I removed the unlabeled mask because i am pretty sure that those are showing the hear\n",
        "        # When i am wrong, its not a big deal for RV Segmentation, because there remain enough\n",
        "        # unlabled masks after setting Myo an lv to zero\n",
        "        # idx_to_delete = []\n",
        "        \n",
        "       \n",
        "        # for index,arr in enumerate(imgs_npy[1]):\n",
        "        #         if not arr.any():\n",
        "        #             idx_to_delete.append(index)   \n",
        "        \n",
        "\n",
        "        # # Removing images without labled mask\n",
        "        # imgs_npy[0]= np.delete(imgs_npy[0],idx_to_delete,0)\n",
        "        # # Removing unlabeled masks\n",
        "        # imgs_npy[1] =np.delete(imgs_npy[1],idx_to_delete,0)\n",
        "        # # get some metadata\n",
        "        \n",
        "        original_shape = imgs_npy[0].shape\n",
        "\n",
        "        # now stack the images into one 4d array, cast to float because we will get rounding problems if we don't\n",
        "        imgs_npy = np.concatenate([i[None] for i in imgs_npy]).astype(np.float32)\n",
        "\n",
        "        # now find the nonzero region and crop to that\n",
        "        nonzero = [np.array(np.where(i != 0)) for i in imgs_npy]\n",
        "        nonzero = [[np.min(i, 1), np.max(i, 1)] for i in nonzero]\n",
        "        nonzero = np.array([np.min([i[0] for i in nonzero], 0), np.max([i[1] for i in nonzero], 0)]).T\n",
        "        # nonzero now has shape 3, 2. It contains the (min, max) coordinate of nonzero voxels for each axis\n",
        "\n",
        "        # now crop to nonzero\n",
        "        imgs_npy = imgs_npy[:,\n",
        "                nonzero[0, 0] : nonzero[0, 1] + 1,\n",
        "                nonzero[1, 0]: nonzero[1, 1] + 1,\n",
        "                nonzero[2, 0]: nonzero[2, 1] + 1,\n",
        "                ]\n",
        "\n",
        "       \n",
        "        # create a thorax mask that we use for normalization\n",
        "        nonzero_masks = [i != 0 for i in imgs_npy[:-1]]\n",
        "        thorax_mask = np.zeros(imgs_npy.shape[1:], dtype=bool)\n",
        "        for i in range(len(nonzero_masks)):\n",
        "            thorax_mask = thorax_mask | nonzero_masks[i]\n",
        "\n",
        "        # minmax normalization using percentiles to be robust against outliers\n",
        "        if self.norm_method == \"min_max\":\n",
        "          for i in range(len(imgs_npy) - 1):\n",
        "              percentile =np.percentile(imgs_npy.ravel(),99)\n",
        "              imgs_npy[i][imgs_npy[i]>percentile] = percentile\n",
        "              max = imgs_npy[i][thorax_mask].max()\n",
        "              min = imgs_npy[i][thorax_mask].min()\n",
        "              imgs_npy[i] = 0 if (max-min) ==0 else (imgs_npy[i] - min) / (max - min)\n",
        "              imgs_npy[i][thorax_mask == 0] = 0\n",
        "        if self.norm_method == \"z_score\":\n",
        "          for i in range(len(imgs_npy) - 1):\n",
        "                mean = imgs_npy[i][thorax_mask].mean()\n",
        "                std = imgs_npy[i][thorax_mask].std()\n",
        "                imgs_npy[i] = (imgs_npy[i] - mean) / (std + 1e-8)\n",
        "                imgs_npy[i][thorax_mask == 0] = 0\n",
        "        # now save as npz\n",
        "        np.save(os.path.join(output_folder, patient_name + \".npy\"), imgs_npy)\n",
        "\n",
        "        metadata = {\n",
        "        'spacing': spacing,\n",
        "        'direction': direction,\n",
        "        'origin': origin,\n",
        "        'original_shape': original_shape,\n",
        "        'nonzero_region': nonzero\n",
        "        }\n",
        "\n",
        "        save_pickle(metadata, join(output_folder, patient_name + \".pkl\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "_zwAW4n97Dk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.   Import needed modules\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy  as np\n",
        "from IPython.display import Image, display\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "import PIL\n",
        "from PIL import ImageOps\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import SimpleITK as sitk\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import plotly.io as pio\n",
        "pd.options.plotting.backend = \"plotly\"\n",
        "pio.renderers.default = \"colab\"\n",
        "from batchgenerators.augmentations.crop_and_pad_augmentations import crop\n",
        "from batchgenerators.augmentations.utils import pad_nd_image\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import math \n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "EitVXEyLrPMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "from batchgenerators.augmentations.crop_and_pad_augmentations import crop\n",
        "from batchgenerators.dataloading.multi_threaded_augmenter import MultiThreadedAugmenter\n",
        "from batchgenerators.examples.brats2017.config import brats_preprocessed_folder, num_threads_for_brats_example\n",
        "from batchgenerators.transforms.abstract_transforms import Compose\n",
        "from batchgenerators.utilities.data_splitting import get_split_deterministic\n",
        "from batchgenerators.utilities.file_and_folder_operations import *\n",
        "import numpy as np\n",
        "from batchgenerators.dataloading.data_loader import DataLoader\n",
        "from batchgenerators.augmentations.utils import pad_nd_image\n",
        "from batchgenerators.transforms.spatial_transforms import SpatialTransform_2, MirrorTransform\n",
        "from batchgenerators.transforms.color_transforms import BrightnessMultiplicativeTransform, GammaTransform\n",
        "from batchgenerators.transforms.noise_transforms import GaussianNoiseTransform, GaussianBlurTransform\n",
        "\n",
        "\n",
        "class ImageLoader2D(DataLoader):\n",
        "    def __init__(self, data, batch_size, patch_size,segmentation_value, num_threads_in_multithreaded=1,seed_for_shuffle=1234, return_incomplete=False,\n",
        "                 shuffle=True,infinite = True):\n",
        "        \"\"\"\n",
        "        data must be a list of patients as returned by get_list_of_patients (and split by get_split_deterministic)\n",
        "        patch_size is the spatial size the retured batch will have\n",
        "        \"\"\"\n",
        "       \n",
        "        super().__init__(data, batch_size, num_threads_in_multithreaded, seed_for_shuffle, return_incomplete, shuffle,\n",
        "                         True)\n",
        "        self.segmentation_value =segmentation_value\n",
        "        self.patch_size = patch_size\n",
        "        self.indices = list(range(len(data)))\n",
        "        self.list_of_used_indices ={}\n",
        "        \n",
        "    @staticmethod\n",
        "    def load_patient(patient):\n",
        "        data = np.load(patient + \".npy\", mmap_mode=\"r\")\n",
        "        # metadata = load_pickle(patient + \".pkl\")\n",
        "        return data#, metadata\n",
        "\n",
        "    def generate_train_batch(self):\n",
        "        # DataLoader has its own methods for selecting what patients to use next, see its Documentation\n",
        "        idx = self.get_indices()\n",
        "        patients_for_batch = [self._data[i] for i in idx]\n",
        "       \n",
        "        # initialize empty array for data and seg\n",
        "        data = np.zeros((self.batch_size,4,*self.patch_size), dtype=np.float32)\n",
        "        seg = np.zeros((self.batch_size,4,*self.patch_size), dtype=np.float32)\n",
        "\n",
        "        metadata = []\n",
        "        patient_names = []\n",
        "\n",
        "        # iterate over patients_for_batch and include them in the batch\n",
        "        for i, j in enumerate(patients_for_batch):\n",
        "            patient_data = self.load_patient(j)\n",
        "\n",
        "            \n",
        "            slice_idx = np.random.choice(patient_data.shape[1])\n",
        "            patient_data = patient_data[:, slice_idx]\n",
        "            \n",
        "            # this will only pad patient_data if its shape is smaller than self.patch_size\n",
        "            patient_data = pad_nd_image(patient_data, self.patch_size)\n",
        "\n",
        "            # now random crop to self.patch_size\n",
        "            # crop expects the data to be (b, c, x, y, z) but patient_data is (c, x, y, z) so we need to add one\n",
        "            # dummy dimension in order for it to work (@Todo, could be improved)\n",
        "            # if len(patient_data.shape)< 5:\n",
        "              #Todo resize instead of cropping\n",
        "            patient_data, patient_seg = crop(patient_data[:-1][None], patient_data[-1:][None], (4,)+self.patch_size, crop_type=\"random\")\n",
        "            \n",
        "\n",
        "            # patient_seg = patient_data[-1:][None]\n",
        "            # patient_data =patient_data[:-1][None]\n",
        "            data[i] = patient_data[0]\n",
        "            seg[i] = patient_seg[0]\n",
        "            seg[i][seg[i]!= self.segmentation_value] =0\n",
        "            \n",
        "        return {'data': data, 'seg':seg}\n",
        "\n"
      ],
      "metadata": {
        "id": "rVkgZRqy7PLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from batchgenerators.utilities.file_and_folder_operations import *\n",
        "class Preprocess():\n",
        "  def __init__(self,list_of_patients,patch_size,threshold,destination_folder):\n",
        "    self.resample(list_of_patients,patch_size,threshold,destination_folder)\n",
        "  @staticmethod\n",
        "  def load_patient(patient):\n",
        "      data = np.load(patient + \".npy\", mmap_mode=\"r\")\n",
        "      metadata = load_pickle(patient + \".pkl\")\n",
        "      return data, metadata\n",
        "  \n",
        "  def resample(self,list_of_patients,new_shape,threshold,destination_folder):\n",
        "   \n",
        "    for i,patient in enumerate(list_of_patients):\n",
        "      data, metadata = self.load_patient(patient)\n",
        "      patient_name = list_of_patients[i].split(\"/\")[-1][0:10]\n",
        "      image = data[0]\n",
        "      segmentation = data[1]\n",
        "      seg = np.zeros((segmentation.shape[0],4,new_shape[1],new_shape[0]),dtype=np.float32)\n",
        "      img = np.zeros((image.shape[0],4,new_shape[1],new_shape[0]),dtype=np.float32)\n",
        "      # Resizing of the image creation of binary segmentation channels\n",
        "      for j,mrt_slice in enumerate(segmentation):\n",
        "        for index,segmentation_value in enumerate([0,1,2,3]):\n",
        "          seg_slice = mrt_slice == segmentation_value\n",
        "          seg[j][index] = cv2.resize(seg_slice.astype(np.float32),new_shape, interpolation = cv2.INTER_AREA)\n",
        "          seg[j][index][ seg[j][index]<threshold] =0\n",
        "          seg[j][index][ seg[j][index]>=threshold] =1\n",
        "        img[j] = cv2.resize(image[j], new_shape,interpolation = cv2.INTER_AREA)\n",
        "        \n",
        "      np_array = np.concatenate([img[None],seg[None]],axis =0).astype(np.float32)\n",
        "      \n",
        "      np.save(os.path.join(destination_folder, patient_name + \".npy\"), np_array)\n",
        "  "
      ],
      "metadata": {
        "id": "zMETchcx7SiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import shutil\n",
        "import natsort\n",
        "import os\n",
        "from batchgenerators.utilities.file_and_folder_operations import *\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    import SimpleITK as sitk\n",
        "except ImportError:\n",
        "    print(\"You need to have SimpleITK installed to run this example!\")\n",
        "    raise ImportError(\"SimpleITK not found\")\n",
        "from multiprocessing import Pool\n",
        "\n",
        "\n",
        "\n",
        "class DivideData():\n",
        "    \n",
        "    \n",
        "    def __init__(self,source_directory,destination_directory,subdirectories):\n",
        "        \"\"\"Initializes a divideData object that reads data from a the srcDir and saves it in provided folder structure\n",
        "        \n",
        "        :param source_directory: path to source folder where the raw data is placed in one folder\n",
        "        :type source_dircetory: str\n",
        "        \n",
        "        :param destination_directory: path to root folder of the subfolders\n",
        "        :type destination_directory: str\n",
        "        \n",
        "        :param subfolders: list of folder names in wich to subdivide the data\n",
        "        :type subfolders: list \n",
        "        \"\"\"\n",
        "        self.load_files(source_directory)\n",
        "        self.divide_data(source_directory,destination_directory,subdirectories)\n",
        "\n",
        "\n",
        "    def load_files(self,source_directory):\n",
        "        \"\"\"Loads files from subfolder to self.files\"\"\"\n",
        "        self.files = os.listdir(source_directory)\n",
        "        self.files.sort()\n",
        "\n",
        "    def divide_data(self,source_directory,destination_directory,subdirectories):\n",
        "        '''Divides data from self.files into provided folderstructure (self.subfolders)'''\n",
        "        files = natsort.natsorted(self.files)\n",
        "\n",
        "        for i,pathologie in enumerate(subdirectories):\n",
        "            pathologie_folder = os.path.join(destination_directory,pathologie)\n",
        "            for index,patient in enumerate(files):\n",
        "                    if int(patient[7:10]) > (i+1)*20:\n",
        "                        files = files[index:]\n",
        "                        break\n",
        "                    if patient[-8:-5] == \"msk\":\n",
        "                        subfolder =\"segmentation\"\n",
        "                    else:\n",
        "                        subfolder =\"image\"\n",
        "                    patient_folder=os.path.join(pathologie_folder,patient[0:10])\n",
        "                    patient_subfolder=os.path.join(patient_folder,subfolder)\n",
        "                    if not os.path.exists(patient_subfolder): \n",
        "                            os.makedirs(patient_subfolder)\n",
        "                    shutil.copy(src=os.path.join(source_directory,patient),dst=patient_subfolder)"
      ],
      "metadata": {
        "id": "kUloDYle7U6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Input, Model\n",
        "from keras.layers import Conv2D, Concatenate, MaxPooling2D\n",
        "from keras.layers import UpSampling2D, Dropout, BatchNormalization\n",
        "\"\"\"\n",
        "Adapted implementation from: https://github.com/pietz/unet-keras/blob/master/unet.py\n",
        "\"\"\"\n",
        "'''\n",
        "U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
        "(https://arxiv.org/abs/1505.04597)\n",
        "---\n",
        "img_shape: (height, width, channels)\n",
        "out_ch: number of output channels\n",
        "start_ch: number of channels of the first conv\n",
        "depth: zero indexed depth of the U-structure\n",
        "increasing_rate: rate at which the conv channels will increase\n",
        "activation: activation function after convolutions\n",
        "dropout: amount of dropout in the contracting part\n",
        "batchnorm: adds Batch Normalization if true\n",
        "maxpool: use strided conv instead of maxpooling if false\n",
        "upconv: use transposed conv instead of upsamping + conv if false\n",
        "residual: add residual connections around each conv block if true\n",
        "'''\n",
        "\n",
        "def convolution_block(inputs, dimension, activation, batchnorm, resudial, dropout=0):\n",
        "\tn = Conv2D(dimension, 3, activation=activation, padding='same')(inputs)\n",
        "\tn = BatchNormalization()(n) if batchnorm else n\n",
        "\tn = Dropout(dropout)(n) if dropout else n\n",
        "\tn = Conv2D(dimension, 3, activation=activation, padding='same')(n)\n",
        "\tn = BatchNormalization()(n) if batchnorm else n\n",
        "\treturn Concatenate()([inputs, n]) if resudial else n\n",
        "\n",
        "def level_block(inputs, dimension, depth, increasing_rate, activation, dropout, batchnorm, maxpooling, up, resudial):\n",
        "\tif depth > 0:\n",
        "\t\tn = convolution_block(inputs, dimension, activation, batchnorm, resudial)\n",
        "\t\tm = MaxPooling2D()(n) if maxpooling else Conv2D(dimension, 3, strides=2, padding='same')(n)\n",
        "\t\tm = level_block(m, int(increasing_rate*dimension), depth-1, increasing_rate, activation, dropout, \n",
        "    batchnorm, maxpooling, up, resudial)\n",
        "\t\tif up:\n",
        "\t\t\tm = UpSampling2D()(m)\n",
        "\t\t\tm = Conv2D(dimension, 2, activation=activation, padding='same')(m)\n",
        "\t\telse:\n",
        "\t\t\tm = Conv2DTranspose(dimension, 3, strides=2, activation=activation, padding='same')(m)\n",
        "\t\tn = Concatenate()([n, m])\n",
        "\t\tm = convolution_block(n, dimension, activation, batchnorm, resudial)\n",
        "\telse:\n",
        "\t\tm = convolution_block(inputs, dimension, activation, batchnorm, resudial, dropout)\n",
        "\treturn m\n",
        "\n",
        "def UNet(img_shape, out_ch=1, start_ch=64, depth=4, increasing_rate=2., activation='relu', \n",
        "\t\t dropout=0.5, batchnorm=True, maxpool=True, upconv=True, residual=False,act_fun_final_conv='sigmoid'):\n",
        "\ti = Input(shape=img_shape)\n",
        "\to = level_block(i, start_ch, depth, increasing_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n",
        "\to = Conv2D(out_ch, 1, activation=act_fun_final_conv)(o)\n",
        "\treturn Model(inputs=i, outputs=o)"
      ],
      "metadata": {
        "id": "LMPIsCSw7ce1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "from time import time\n",
        "from batchgenerators.augmentations.crop_and_pad_augmentations import crop\n",
        "from batchgenerators.dataloading.multi_threaded_augmenter import MultiThreadedAugmenter\n",
        "from batchgenerators.examples.brats2017.config import brats_preprocessed_folder, num_threads_for_brats_example\n",
        "from batchgenerators.transforms.abstract_transforms import Compose\n",
        "from batchgenerators.utilities.data_splitting import get_split_deterministic\n",
        "from batchgenerators.utilities.file_and_folder_operations import *\n",
        "import numpy as np\n",
        "import natsort\n",
        "from batchgenerators.utilities.file_and_folder_operations import *\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from batchgenerators.transforms.spatial_transforms import SpatialTransform_2, MirrorTransform\n",
        "from batchgenerators.transforms.sample_normalization_transforms import RangeTransform, CutOffOutliersTransform\n",
        "from batchgenerators.transforms.color_transforms import BrightnessMultiplicativeTransform, GammaTransform\n",
        "from batchgenerators.transforms.noise_transforms import GaussianNoiseTransform, GaussianBlurTransform\n",
        "def get_list_of_patients(preprocessed_data_folder):\n",
        "        npy_files = subfiles(preprocessed_data_folder, suffix=\".npy\", join=True)\n",
        "        # remove npy file extension\n",
        "        patients = [i[:-4] for i in npy_files]\n",
        "        return natsort.natsorted(patients)\n",
        "\n",
        "\n",
        "def get_split_deterministic(all_keys, fold=0, num_splits=5, random_state=12345):\n",
        "    \"\"\"\n",
        "    Splits a list of patient identifiers (or numbers) into num_splits folds and returns the split for fold fold.\n",
        "    :param all_keys:\n",
        "    :param fold:\n",
        "    :param num_splits:\n",
        "    :param random_state:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    all_keys_sorted = np.sort(list(all_keys))\n",
        "    splits = KFold(n_splits=num_splits, shuffle=True, random_state=random_state)\n",
        "    for i, (train_idx, test_idx) in enumerate(splits.split(all_keys_sorted)):\n",
        "        if i == fold:\n",
        "            train_keys = np.array(all_keys_sorted)[train_idx]\n",
        "            test_keys = np.array(all_keys_sorted)[test_idx]\n",
        "            break\n",
        "    return train_keys, test_keys\n",
        "def get_train_transform(patch_size):\n",
        "    # we now create a list of transforms. These are not necessarily the best transforms to use for BraTS, this is just\n",
        "    # to showcase some things\n",
        "    tr_transforms = []\n",
        "\n",
        "    # Elastic deform, rotation sacling, and randomcropping with a probabilty of 0.2 each \n",
        "    tr_transforms.append(\n",
        "          SpatialTransform_2(\n",
        "            patch_size, [i // 2 for i in patch_size],\n",
        "            do_elastic_deform=True, deformation_scale=(0, 0.25),\n",
        "            do_rotation=True,\n",
        "            angle_x=(- 15 / 360. * 2 * np.pi, 15 / 360. * 2 * np.pi),\n",
        "            angle_y=(- 15 / 360. * 2 * np.pi, 15 / 360. * 2 * np.pi),\n",
        "            angle_z=(- 15 / 360. * 2 * np.pi, 15 / 360. * 2 * np.pi),\n",
        "            do_scale=True, scale=(0.75, 1.25),\n",
        "            border_mode_data='constant', border_cval_data=0,\n",
        "            border_mode_seg='constant', border_cval_seg=0,\n",
        "            order_seg=0, order_data=3,\n",
        "            random_crop=True,\n",
        "            p_el_per_sample=0.2, p_rot_per_sample=0.2, p_scale_per_sample=0.2\n",
        "        )\n",
        "    )\n",
        "    tr_transforms.append(CutOffOutliersTransform())\n",
        "    \n",
        "    tr_transforms.append(RangeTransform((0, 1), True, data_key=\"data\", label_key=\"seg\"))\n",
        "\n",
        "\n",
        "    tr_transforms = Compose(tr_transforms)\n",
        "    return tr_transforms\n",
        "def get_val_transform(patch_size):\n",
        "  tr_transforms = []\n",
        "  tr_transforms.append(CutOffOutliersTransform())\n",
        "    \n",
        "  tr_transforms.append(RangeTransform((0, 1), True, data_key=\"data\", label_key=\"seg\"))\n",
        "\n",
        "\n",
        "  tr_transforms = Compose(tr_transforms)\n",
        "  return tr_transforms\n"
      ],
      "metadata": {
        "id": "XEqjGiJ97iqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Colab specific settings\n",
        "\n",
        "> 1. Checking if GPU is enabled\n",
        "> 2. Mounting Google Drive, to get the created python modules\n",
        "\n"
      ],
      "metadata": {
        "id": "egp4GjREuoG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Check if GPU is enabled\n",
        "# %tensorflow_version 2.x\n",
        "# import tensorflow as tf\n",
        "# device_name = tf.test.gpu_device_name()\n",
        "# if device_name != '/device:GPU:0':\n",
        "#   raise SystemError('GPU device not found')\n",
        "# print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "3jXWR-4muTP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTGk3xsHayGo"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setting up python environment\n",
        "\n",
        "\n",
        "1.   Install requiered modules\n",
        "2.   Import needed modules\n",
        "3.   Add bin to system path\n",
        "4.   Import local modules from bin\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kPzxQji-t0Ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.   Add bin to system path\n",
        "root_dir=\"\"\n",
        "# append root_dir to systempath to make local imports possible\n",
        "module_dir = os.path.join(root_dir,\"bin\")\n",
        "sys.path.append(module_dir)"
      ],
      "metadata": {
        "id": "N09-03SquQI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Get the Data\n",
        "1. Download the data\n",
        "2. Unzip the data"
      ],
      "metadata": {
        "id": "KSdaSEyywRZd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8O2lpL9TUHW"
      },
      "source": [
        "# 1. Download and unzip  the data\n",
        "\n",
        "data_dir =  os.path.join(root_dir,\"data\")\n",
        "if not os.path.isdir(data_dir):\n",
        "  os.makedirs(data_dir)\n",
        "  !wget -O data.html    https://heibox.uni-heidelberg.de/f/eef4cf141a4746d2bf45/?dl=1\n",
        "  !unzip   -q 'data.html' -d \"/data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stratification of the data\n",
        "1. Define directory paths\n",
        "2. Divide data into the provided subfolders"
      ],
      "metadata": {
        "id": "xSg-TaPDwvGn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBg2_azqQBgZ"
      },
      "source": [
        "# 1. Define directory paths\n",
        "# To have a leightweight implementation we only want to have the current batch in RAM. Therefore\n",
        "# everything else is stored on the harddrive\n",
        "raw_data_dir = os.path.join(root_dir,\"/data/all\")\n",
        "destination_dir = os.path.join(root_dir,\"/data/dividedData\")\n",
        "np_data_dir = os.path.join(root_dir,\"/data/numpy\")\n",
        "np_preprocessed_data_dir = os.path.join(root_dir,\"/data/np_preprocessed\")\n",
        "subfolder = [\"DCM\",\"HCM\",\"MINF\",\"NOR\",\"RV\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4bYywhwfrdV"
      },
      "source": [
        "# 2. Divide data into the provided dirStructure\n",
        "# For every category (\"DCM\",\"HCM\",\"MINF\",\"NOR\",\"RV\") one folder is created where the corresponding nrrd files are stored\n",
        "if not os.path.isdir(destination_dir):\n",
        "  dataDivider = DivideData(raw_data_dir,destination_dir,subfolder)\n",
        "else: \n",
        "  print(\"The directory: {} already exists, therefore the data Division is not\\\n",
        "  done.\\n If you want to load the data again delete\\\n",
        "  the directory: {}\".format(destination_dir,destination_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing\n",
        "1. Load data from nrrd into numpy Array\n",
        "2.Get an overview of the data\n",
        "*The implementation of the data preprocessing and augmentation is heavely based on the BraTS2017/2018 example provided by MIC-DKFZ: [Example Batchgenerators](https://github.com/MIC-DKFZ/batchgenerators/tree/master/batchgenerators/examples/brats2017)*"
      ],
      "metadata": {
        "id": "XxnrgAQ7xY9w"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhQPPUD5ev42"
      },
      "source": [
        "# 1. Load data from nrrd into numpy Array\n",
        "# With attached google drive this step sometimes takes ages\n",
        "# We stack the slices for each patien into a 4 D image First dimension is image/segmnetation\n",
        "# second dimension is the channel, 3rd and 4th the image/seg dimensions\n",
        "# Also a min_max normalization is done for performance reasons (after data augmentation another one is done)\n",
        "if not os.path.isdir(np_data_dir):\n",
        "  numpyGenerator = CreateNumpyArr(destination_dir,np_data_dir,subfolder,\"min_max\")\n",
        "else: \n",
        "  print(\"The directory: {} already exists, therefore the numpy array are not\\\n",
        "  created.\\n If you want to load the data again delete\\\n",
        "  the directory: {}\".format(np_data_dir,np_data_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Patient 6 [0,11]\n",
        "# Patient 1 [0,10]\n",
        "# Patient 12 [20]\n",
        "# Patient 21 [10]\n",
        "# Patient 23 [8,17]\n",
        "# Patient 26 [10]\n",
        "# Patient 27 [10]\n",
        "# Patint 28 [9, 19]\n",
        "# Patient 29[11, 12, 21]\n",
        "# Patiennt 30 [0, 10, 19]\n",
        "# Patient 31 [10, 19]\n",
        "# Patient 32[10]\n",
        "# Pateint 33 [0, 10, 11, 19]\n",
        "# plt.rcParams['figure.figsize'] = [40, 40]\n",
        "# plt.subplot(2,2,1)\n",
        "# plt.imshow(np.load(\"/content/drive/MyDrive/ADVMB/data/numpy/patient006.npy\")[0][0])\n",
        "# plt.subplot(2,2,2)\n",
        "# plt.imshow(np.load(\"/content/drive/MyDrive/ADVMB/data/numpy/patient006.npy\")[0][1])\n",
        "# plt.subplot(2,2,3)\n",
        "# plt.imshow(np.load(\"/content/drive/MyDrive/ADVMB/data/numpy/patient006.npy\")[1][0])\n",
        "# plt.subplot(2,2,4)\n",
        "# plt.imshow(np.load(\"/content/drive/MyDrive/ADVMB/data/numpy/patient006.npy\")[1][1])"
      ],
      "metadata": {
        "id": "4nxrF4D855un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Get an overview of the data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1.   Count total amount of slices (2D-images)\n",
        "2.   Plot histogram on a subset of slices\n",
        "3. Get maximum space\n",
        "3. Resize images\n",
        "3.   Get the image dimensions\n",
        "4.   Plot histogram of image dimensions\n",
        "5.   Choose the patch_size\n",
        "\n"
      ],
      "metadata": {
        "id": "R-DHlhZFShGn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy-0DYDo411G"
      },
      "source": [
        "# 1. Count total amount of slices (2D-images)\n",
        "number_of_total_slices = 0 \n",
        "hist_list = []\n",
        "slice_nr = 5\n",
        "for patient in os.listdir(np_data_dir):\n",
        " if patient.endswith(\".npy\"):\n",
        "   path_to_patient = os.path.join(np_data_dir,patient)\n",
        "   patient_data = np.load(path_to_patient)\n",
        "   hist_of_patient = patient_data[0][slice_nr].ravel()\n",
        "   hist_list.append(hist_of_patient)\n",
        "   number_of_total_slices += patient_data.shape[1]\n",
        "   \n",
        "print(\"The total number slices is: {}\".format(number_of_total_slices))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot one example slice to ensure that the right data is loaded\n",
        "plt.rcParams['figure.figsize'] = [10, 10]\n",
        "plt.imshow(patient_data[0][0],cmap=\"gray\")"
      ],
      "metadata": {
        "id": "S_7qYXevCzey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Plot histogram on a subset of slices\n",
        "\n",
        "# Did a min-max normalization while creating the images\n",
        "plt.rcParams['figure.figsize'] = [50, 50]\n",
        "fig, axs = plt.subplots(10,10, figsize=(50, 50), facecolor='w', edgecolor='k')\n",
        "fig.subplots_adjust(hspace = 0.2, wspace=.2)\n",
        "axs = axs.ravel()\n",
        "\n",
        "for i in range(100):\n",
        "    axs[i].hist([hist_list[i]])\n",
        "    axs[i].set_title(str(i))"
      ],
      "metadata": {
        "id": "NQCdvgpZxBad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Get the image dimensions\n",
        "patients =  get_list_of_patients(np_data_dir)\n",
        "image_dimensions_x =[]\n",
        "image_dimensions_y =[]\n",
        "for patient in patients:\n",
        "  arr = np.load(os.path.join(np_data_dir,patient+\".npy\"))\n",
        "  image_shapes = arr.shape[0]\n",
        "  image_dimensions_x.append(arr.shape[2])\n",
        "  image_dimensions_y.append(arr.shape[3])\n"
      ],
      "metadata": {
        "id": "Yk9JCk6-y7eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Plot histogram of image dimensions \n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Histogram(x=image_dimensions_x,name=\"X-dim\"))\n",
        "fig.add_trace(go.Histogram(x=image_dimensions_y,name=\"Y-dim\"))\n",
        "\n",
        "# Overlay both histograms\n",
        "fig.update_layout(barmode='overlay')\n",
        "# Reduce opacity to see both histograms\n",
        "fig.update_traces(opacity=0.75)\n",
        "\n",
        "fig.update_layout(\n",
        "    title_text='Plot of dimensions', # title of plot\n",
        "    xaxis_title_text='Value', # xaxis label\n",
        "    yaxis_title_text='Count', # yaxis label\n",
        "    bargap=0.2, # gap between bars of adjacent location coordinates\n",
        "    bargroupgap=0.1 # gap between bars of the same location coordinates\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "Lg-nrlFVWyYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6DpBbAgDMMHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Choose the patch size\n",
        "min_x = min(image_dimensions_x)\n",
        "min_y = min(image_dimensions_y)\n",
        "minimum_size = min(min_x,min_y)\n",
        "next_min = math.floor(math.log(minimum_size,2),)\n",
        "patch_size = (2**next_min, 2**next_min)\n",
        "\n",
        "\n",
        "patch_size\n"
      ],
      "metadata": {
        "id": "Nxd_uNXwXkwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Applying resizing to all data, because we don't want to handle training and dataset different.\n",
        "if not os.path.isdir(np_preprocessed_data_dir):\n",
        "  os.makedirs(np_preprocessed_data_dir)\n",
        "  preproccessNp =Preprocess(patients,patch_size,0.5,np_preprocessed_data_dir)\n",
        "\n",
        "else: \n",
        "  print(\"The directory: {} already exists, therefore the numpy array are not\\\n",
        "  created.\\n If you want to load the data again delete\\\n",
        "  the directory: {}\".format(np_data_dir,np_data_dir))"
      ],
      "metadata": {
        "id": "0DSobkQXMjzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting Data\n",
        "1.   Create Train,Test and Validation Set\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fw_JiHYiHTf4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKDkn3UkfFwu"
      },
      "source": [
        "# 1.   Create Train,Test and Validation Set\n",
        "dir = np_preprocessed_data_dir\n",
        "patients =  get_list_of_patients(dir)\n",
        "train_list=[]\n",
        "val_list=[]\n",
        "test_list= []\n",
        "# From each category [\"DCM\",\"HCM\",\"MINF\",\"NOR\",\"RV\"] assign 80 % of the\n",
        "# data to the trainset, and 10% to val, and test set each\n",
        "for i in range(0,100,20):\n",
        "  trainSub, valSub = get_split_deterministic(patients[i:(i+20)], fold=0, num_splits=5, random_state=100)  \n",
        "  valSub,testSub = get_split_deterministic(valSub,fold=0,num_splits=2, random_state=100)\n",
        "  train_list.extend(trainSub)\n",
        "  val_list.extend(valSub)\n",
        "  test_list.extend(testSub)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the training\n",
        "\n",
        "\n",
        "1.   Initiate ImageLoaser for train an validation\n",
        "2.   Define transformations\n",
        "3.   Initiate the MultiThreadedAugmenters\n",
        "4. Plot histograms and images of one batch, and check if results are reasonable  \n",
        "5. Plot images of one batch, and check if results are reasonable\n"
      ],
      "metadata": {
        "id": "K1UYQvSsTieI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.   Initiate ImageLoader for train an validation**\n"
      ],
      "metadata": {
        "id": "E-1eZdTiUzFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size =16\n",
        "# seg_values =[1] # \n",
        "dataloader_train = ImageLoader2D(data=train_list, batch_size=batch_size, patch_size=patch_size,segmentation_value =1)\n",
        "dataloader_validation =ImageLoader2D(data=val_list, batch_size=batch_size, patch_size=patch_size, segmentation_value =1)"
      ],
      "metadata": {
        "id": "3Mpi72Nj73aW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Define transformations**"
      ],
      "metadata": {
        "id": "WwAHrIn6VBky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tr_transforms = get_train_transform(patch_size)"
      ],
      "metadata": {
        "id": "iul0jTSH7r4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_transforms = get_val_transform(patch_size)"
      ],
      "metadata": {
        "id": "aURyvbmSo4p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Initiate and start MultiThreadedAugmenter**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BHPPyMqWVNWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Maybe need to adjust num_processes when aother server is used to execute the script\n",
        "tr_gen = MultiThreadedAugmenter(dataloader_train, tr_transforms, num_processes=1,\n",
        "                                num_cached_per_queue=1,\n",
        "                                seeds=None, pin_memory=False)\n",
        "val_gen = MultiThreadedAugmenter(dataloader_validation, val_transforms,\n",
        "                                    num_processes=1, num_cached_per_queue=1,\n",
        "                                    seeds=None,\n",
        "                                    pin_memory=False)"
      ],
      "metadata": {
        "id": "-U8fn6jcy4bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_gen.restart()"
      ],
      "metadata": {
        "id": "tmT7APBo7vsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_gen.restart()"
      ],
      "metadata": {
        "id": "108DAD_W7xhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(tr_gen)"
      ],
      "metadata": {
        "id": "2JwD36O8iB0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Plot images of one batch, and check if results are reasonable**"
      ],
      "metadata": {
        "id": "SnYn-oMJVblT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Batchviewer ist a tool that enables to quickly visualize a batch.  Through the visualization it was noticed that some slices were not labeled, these were removed accordingly in the preprocessing. It was not possible to install Batchviewer on Google colab, therefore it is not used here. Instead the images of one batch are plotted with matplotlib**"
      ],
      "metadata": {
        "id": "qZv8-3mktAvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize'] = [20, 20]\n",
        "fig, axs = plt.subplots(batch_size//4,4,figsize=(40,40))\n",
        "\n",
        "fig.subplots_adjust(hspace = 0.2, wspace=.2)\n",
        "axs = axs.ravel()\n",
        "# Display mask for RV\n",
        "for i,image in enumerate(batch[\"seg\"]):\n",
        "    axs[i].imshow(image[1],cmap=\"gray\")\n",
        "    axs[i].set_title(str(i))"
      ],
      "metadata": {
        "id": "p7CPxUdLVr6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Plot histograms of one batch, and check if results are reasonable**"
      ],
      "metadata": {
        "id": "GzyUWTierfiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(tr_gen)\n",
        "plt.rcParams['figure.figsize'] = [10, 10]\n",
        "fig, axs = plt.subplots(batch_size//2,2, figsize=(20, 20), facecolor='w', edgecolor='k')\n",
        "fig.subplots_adjust(hspace = 0.2, wspace=.2)\n",
        "axs = axs.ravel()\n",
        "hist_list = []\n",
        "for patient_slice in batch[\"data\"]:\n",
        "   hist_of_patient = patient_slice.ravel()\n",
        "   hist_list.append(hist_of_patient)\n",
        "for i in range(batch_size):\n",
        "    axs[i].hist([hist_list[i]])\n",
        "    axs[i].set_title(str(i))"
      ],
      "metadata": {
        "id": "-HlPVIGeG2hE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNet\n",
        "\n",
        "\n",
        "1.   **Build the model**\n",
        "2.   **Callback Mehtod**\n",
        "\n"
      ],
      "metadata": {
        "id": "_UNkLTpDZVxx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1 .Build the model**"
      ],
      "metadata": {
        "id": "eBPI55nsrz6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "keras.backend.clear_session()\n",
        "\n",
        "num_classes =1\n",
        "# Build model\n",
        "\n",
        "img_size =patch_size+(num_classes,)\n",
        "model = UNet(img_shape=img_size, out_ch=1, start_ch=64, depth=4, increasing_rate=2., activation='relu', \n",
        "\t\t dropout=0.5, batchnorm=True, maxpool=True, upconv=True, residual=False)\n",
        "# model.summary()\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch < 5:\n",
        "    return lr\n",
        "  else:\n",
        "    return lr * tf.math.exp(-0.1)\n",
        "model.summary()\n",
        "opt = Adam(learning_rate =0.0001)\n",
        "model.compile(optimizer=opt, loss='binary_crossentropy')\n"
      ],
      "metadata": {
        "id": "Qgm3Thxepcqo",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Callback function**"
      ],
      "metadata": {
        "id": "kOEkqdnMrm6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def plotPred(model,val,shape,avgTrainLoss,avgValLoss):\n",
        "  \n",
        "  print(\"Epoch results\")\n",
        "  print(\"Average training loss over batches in epoch: {}\".format(avgTrainLoss))\n",
        "  print(\"Average validation loss over batches in epoch: {}\".format(avgValLoss))\n",
        "  for i in range(0,3):\n",
        "    plt.subplot(3,3,(i*3)+1)\n",
        "    pred = model.predict(val[0])\n",
        "    pred = pred.reshape(shape)\n",
        "    X_val = val[0].reshape(shape)\n",
        "    y_val = val[1].reshape(shape)\n",
        "    plt.imshow(X_val[i][0],cmap=\"gray\")\n",
        "    plt.title(\"Original\")\n",
        "    plt.subplot(3,3,(i*3)+2)\n",
        "    plt.imshow(y_val[i][0],cmap=\"gray\")\n",
        "    plt.title(\"Ground Truth\")\n",
        "    plt.subplot(3,3,(i*3)+3)\n",
        "    plt.imshow(pred[i][0],cmap=\"gray\")\n",
        "    plt.title(\"Predicted Segmentation\")\n",
        "    plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "yin0RDGEnqzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training of the UNet\n",
        "\n",
        "\n",
        "1.   Training\n",
        "2.   Save model and training data\n",
        "3.   Plot the results\n",
        "\n"
      ],
      "metadata": {
        "id": "3NTCgtNnZhio"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Training**"
      ],
      "metadata": {
        "id": "RaHcDgzdsEtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Not to pretty implemetation of the epochs, but i had no idea how \n",
        "# to write an traininggenerator for keras based on the batchgenerator\n",
        "# The creation of the batches is random which means that it is possible that one slice,\n",
        "# can be seen more than one time at each epoch and another is not presented to the model \n",
        "# in that epoch. In this case it isn't too much pain because over 20 Epochs one can assume\n",
        "# that the random choose of the batch has not too much effet.\n",
        "plt.rcParams['figure.figsize'] = [20, 10]\n",
        "total_history = {\"loss\":[],\"val_loss\":[]}\n",
        "epoch_history = {\"loss\":[],\"val_loss\":[]}\n",
        "num_epochs = 20\n",
        "num_batches_per_epoch =number_of_total_slices//batch_size\n",
        "new_shape = (16,)+patch_size+(1,)\n",
        "old_shape = (16,)+(1,)+patch_size\n",
        "seg_channel = 1 #seg --> 0: Background, 1:RV cavity, 2: Myocardium, 3:LV-Cavity\n",
        "for epoch in range(num_epochs):\n",
        "  print(\"Started epoch: {}\".format(epoch+1))\n",
        "  for b in range(num_batches_per_epoch):\n",
        "      batchTrain = next(tr_gen)\n",
        "     \n",
        "      X_train = np.reshape(batchTrain[\"data\"][:,seg_channel],new_shape)\n",
        "      y_train = np.reshape(batchTrain[\"seg\"][:,seg_channel],new_shape)\n",
        "      batchVal = next(val_gen)\n",
        "      # Segmentation of right ventricel is in first channel\n",
        "      X_val = np.reshape(batchVal[\"data\"][:,seg_channel],new_shape)\n",
        "      y_val = np.reshape(batchVal[\"seg\"][:,seg_channel],new_shape)\n",
        "      val = [X_val,y_val]\n",
        "      history_of_batch = model.fit(x=X_train,y=y_train,validation_data=val,  \n",
        "                                   epochs=1,batch_size=batch_size,verbose=0)\n",
        "      epoch_history[\"loss\"].extend(history_of_batch.history[\"loss\"])\n",
        "      epoch_history[\"val_loss\"].extend(history_of_batch.history[\"val_loss\"])\n",
        "      \n",
        "      \n",
        "      if epoch ==0:\n",
        "        val_1 = val\n",
        "  avgTrainLoss = sum(epoch_history[\"loss\"])/len(epoch_history[\"loss\"]) \n",
        "  avgValLoss = sum(epoch_history[\"val_loss\"])/len(epoch_history[\"val_loss\"]) \n",
        "  plotPred(model,val_1,old_shape,avgTrainLoss,avgValLoss)\n",
        "  \n",
        "  total_history[\"loss\"].append(avgTrainLoss)\n",
        "  total_history[\"val_loss\"].append(avgValLoss)\n",
        "  epoch_history = {\"loss\":[],\"val_loss\":[]}\n",
        "  history_path = os.path.join(root_dir,\"/model/history.pkl\")"
      ],
      "metadata": {
        "id": "jJFBpUKJyaYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Save model and training data**"
      ],
      "metadata": {
        "id": "UaVneLgBaPyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = os.path.join(root_dir,\"model\")\n",
        "historyPath = os.path.join(root_dir,\"/model/history.pkl\")\n",
        "os.makedirs(os.path.dirname(historyPath), exist_ok=True)\n",
        "\n",
        "\n",
        "with open(history_path, 'wb') as f:\n",
        "    pickle.dump(total_history, f)\n",
        "with open(historyPath, 'rb') as f:\n",
        "    total_history= pickle.load(f)\n",
        "\n",
        "model.save(model_path)"
      ],
      "metadata": {
        "id": "D_wcARVyfPKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model =keras.models.load_model(\"/content/drive/MyDrive/ADVMB/model\")"
      ],
      "metadata": {
        "id": "OKBZ2iI_ff9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(historyPath, 'rb') as f:\n",
        "    total_history= pickle.load(f)"
      ],
      "metadata": {
        "id": "OJYaegujFJL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Plot the results**"
      ],
      "metadata": {
        "id": "3oQEsY_DaloS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(total_history)\n"
      ],
      "metadata": {
        "id": "FhvBUok-K5iG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fig=df.plot(labels=dict(value=\"binary cross entropy\", index=\"epoch\",x_ticks=[1,2]))\n",
        "fig.update_layout(\n",
        "    margin=dict(l=20, r=20, t=20, b=20),\n",
        "    autosize=False,\n",
        "    width=1000,\n",
        "    height=500,\n",
        "\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "4qfzsKFa6e4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of the model\n",
        "1. Make Prediction on the hole training set and plot a few examples"
      ],
      "metadata": {
        "id": "Hd6UXl8HbAB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_patient(patient):\n",
        "      data = np.load(patient + \".npy\")\n",
        "      return data"
      ],
      "metadata": {
        "id": "yt9JsgVpNPtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vC5Y1h1QBgi"
      },
      "source": [
        "from batchgenerators.augmentations.normalizations import cut_off_outliers, mean_std_normalization, range_normalization, \\\n",
        "    zero_mean_unit_variance_normalization,min_max_normalization\n",
        "patch_size=(128,128)\n",
        "from medpy.metric.binary import dc\n",
        "dices = []\n",
        "threshold =0.8\n",
        "plt.rcParams['figure.figsize'] = [20, 10]\n",
        "imgOutputPath = os.path.join(root_dir,\"images_RV\")\n",
        "os.makedirs(imgOutputPath, exist_ok=True)\n",
        "new_shape =(128,128)\n",
        "\n",
        "for i, patient in enumerate(test_list):\n",
        "    patient_name =patient.split(\"/\")[-1][0:10]\n",
        "    data = load_patient(os.path.join(np_preprocessed_data_dir,patient_name))\n",
        "    data = data[:,:,1,:,:]\n",
        "    image = data[0] \n",
        "    segmentation = data[1]\n",
        "    pred = image[None]\n",
        "    pred = pred.reshape((image.shape[0],*patch_size,1))\n",
        "    pred = model.predict(pred)\n",
        "    pred = pred.reshape((1,image.shape[0],*patch_size))\n",
        "    for index,mrt_slice in enumerate(image):\n",
        "      \n",
        "      if index ==5:\n",
        "        plt.subplot(1,4,1)\n",
        "        plt.title(\"MRT\")\n",
        "        plt.imshow(mrt_slice,cmap=\"gray\")\n",
        "        \n",
        "        plt.subplot(1,4,2)\n",
        "        plt.title(\"Segmentation\")\n",
        "        plt.imshow(segmentation[index],cmap =\"gray\")\n",
        "        \n",
        "        plt.subplot(1,4,3)\n",
        "        plt.title(\"Prediction\")\n",
        "        plt.imshow(pred[0][index],cmap =\"gray\")\n",
        "        \n",
        "        threshold = 0.6\n",
        "        pred[pred>=threshold] = 1\n",
        "        pred[pred<threshold] =0\n",
        "     \n",
        "        dice =dc(pred[0][index],segmentation[index])\n",
        "        plt.subplot(1,4,4)\n",
        "        plt.title(\"Prediction with threshold, SDC: {}\".format(round(dice,3)))\n",
        "        plt.imshow(pred[0][index],cmap =\"gray\")\n",
        "        \n",
        "        # plt.savefig(\"/content/\"+str(i)+\"_\"+str(index)+\".pdf\",bbox_inches='tight')\n",
        "        plt.show()\n",
        "      pred[pred>=threshold] = 1\n",
        "      pred[pred<threshold] =0\n",
        "# Because there are many unlabled mask after removing the mask for myo an LV the dice is only calculated when a mask is present\n",
        "      \n",
        "      if not np.sum(segmentation[index])==0:\n",
        "         dices.append(dc(pred[0][index],segmentation[index]))\n",
        "dices = np.array(dices)\n",
        "mean =round(dices.mean(),5)\n",
        "std =round(dices.std(),5)\n",
        "print(mean,std)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dices = []\n",
        "for i, patient in enumerate(val_list):\n",
        "    patient_name =patient.split(\"/\")[-1][0:10]\n",
        "    data = load_patient(os.path.join(np_preprocessed_data_dir,patient_name))\n",
        "    data = data[:,:,1,:,:]\n",
        "    image = data[0] \n",
        "    segmentation = data[1]\n",
        "    pred = image[None]\n",
        "    pred = pred.reshape((image.shape[0],*patch_size,1))\n",
        "    pred = model.predict(pred)\n",
        "    pred = pred.reshape((1,image.shape[0],*patch_size))\n",
        "    for index,mrt_slice in enumerate(image):\n",
        "      pred[pred>=threshold] = 1\n",
        "      pred[pred<threshold] =0\n",
        "# Because there are many unlabled mask after removing the mask for myo an LV the dice is only calculated when a mask is present\n",
        "    \n",
        "      if not np.sum(segmentation[index])==0:\n",
        "        dices.append(dc(pred[0][index],segmentation[index]))\n",
        "      \n",
        "dices = np.array(dices)\n",
        "mean =round(dices.mean(),5)\n",
        "std =round(dices.std(),5)\n",
        "print(mean,std)\n"
      ],
      "metadata": {
        "id": "AJyG6ZZPpC-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dices = []\n",
        "for i, patient in enumerate(train_list):\n",
        "    patient_name =patient.split(\"/\")[-1][0:10]\n",
        "    data = load_patient(os.path.join(np_preprocessed_data_dir,patient_name))\n",
        "    data = data[:,:,1,:,:]\n",
        "    image = data[0] \n",
        "    segmentation = data[1]\n",
        "    pred = image[None]\n",
        "    pred = pred.reshape((image.shape[0],*patch_size,1))\n",
        "    pred = model.predict(pred)\n",
        "    pred = pred.reshape((1,image.shape[0],*patch_size))\n",
        "    for index,mrt_slice in enumerate(image):\n",
        "      pred[pred>=threshold] = 1\n",
        "      pred[pred<threshold] =0\n",
        "# Because there are many unlabled mask after removing the mask for myo an LV the dice is only calculated when a mask is present\n",
        "      if not np.sum(segmentation[index])==0:\n",
        "        dices.append(dc(pred[0][index],segmentation[index]))\n",
        "        \n",
        "       \n",
        "dices = np.array(dices)\n",
        "mean =round(dices.mean(),5)\n",
        "std =round(dices.std(),5)\n",
        "print(mean,std)"
      ],
      "metadata": {
        "id": "8w9Tji0Vh1hf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}